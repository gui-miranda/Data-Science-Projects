{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Books Promotions Recommendation Algorithm\n",
    "\n",
    "* OBJECTIVE :\n",
    "  \n",
    "  In this project my objective was create a machine learning solution that will help users or small companies to filter the   best books promotions according to theirs personal interests on the Brazilian website \"Mercado Livre\".\n",
    "\n",
    "* SOLUTION STEPS OF CONSTRUCTION :\n",
    "  \n",
    "  * Make the Web Scrapping\n",
    "  \n",
    "  * Process and Clean the collected Data\n",
    "  \n",
    "  * Build,Optimize and Validate diferents ML algorithms  : Random Forest,Light GBM and Logistical Regression\n",
    "  \n",
    "  * Merge the best models in a single ensanble with better performance\n",
    "  \n",
    "  * Make the deploy of the final ensamble\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping \n",
    "\n",
    "* In this step I initially scraped the links ofthe  advertisements from the main searchpage for each query, then I accessed the advertisements webpage and extracted the * Price , Author , Title , Total Sale , Publisher ,Language and Format * information.\n",
    "\n",
    "* After finished the scraping, I saved all the data in a CSV file to add the properly classification of the ads.            (Non-Interasting : 0 or Interasting : 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time \n",
    "\n",
    "import requests as rq\n",
    "import bs4 as bs4\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colleting the links of each adverstaisment from the main webpage \n",
    "# Selected queries\n",
    "queries = [\"financas\",'ficcao-cientifica','exatas']\n",
    "\n",
    "# Collecting the data from the main search page for each query  \n",
    "for query in queries:\n",
    "    for i in range(1,21):\n",
    "        if i == 1:\n",
    "            url =(f'https://livros.mercadolivre.com.br/livros/{query}')  \n",
    "        else:\n",
    "            page = (48*i - 47)\n",
    "            url =(f'https://livros.mercadolivre.com.br/livros/{query}/_Desde_{page}')\n",
    "\n",
    "        print(url)\n",
    "        \n",
    "        # Creating a HTML file to save all the data scraped \n",
    "        response = rq.get(url)\n",
    "        with open(f'{query}_pg.{i}.html', 'w+',encoding='utf-8') as output:\n",
    "            output.write(response.text)\n",
    "            time.sleep(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the individual data From each advertsiment \n",
    "for query in queries:\n",
    "    for i in range(1,21):\n",
    "            \n",
    "        with open(f'{query}_pg.{i}.html','r+',encoding='utf-8') as inp:\n",
    "            page_html = inp.read()\n",
    "            \n",
    "            parsed = bs4.BeautifulSoup(page_html)\n",
    "            \n",
    "            tags = parsed.findAll('a')\n",
    "            \n",
    "            for e in tags:\n",
    "                \n",
    "                if e.has_attr('class') and e.has_attr('title'):\n",
    "            \n",
    "                    if e['class'] == ['ui-search-link']:\n",
    "                   \n",
    "                        link = e['href']\n",
    "                        title = e['title']\n",
    "               \n",
    "                \n",
    "                        with open(\"parsed_videos.json\", 'a+') as output:\n",
    "                            data = {\"link\": link, \"title\": title,\"query\" : query}\n",
    "                            output.write(\"{}\\n\".format(json.dumps(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in lista_links:\n",
    "    \n",
    "    try:\n",
    "        with open(f'{link[36:49]}.html','r+',encoding='utf-8') as inp:\n",
    "                    page_html = inp.read()\n",
    "            \n",
    "                    parsed = bs4.BeautifulSoup(page_html)\n",
    "            \n",
    "                    # Collecting the data of the Prices \n",
    "                    class_price = parsed.find_all(attrs={\"itemprop\":re.compile(r\"price\")})[0]\n",
    "                    price.append(float(class_price['content']))\n",
    "            \n",
    "           \n",
    "                    # Collecting the data of the Total Sales\n",
    "                    try:\n",
    "                        class_sales = parsed.find_all(attrs={\"class\":re.compile(r\"ui-pdp-subtitle\")})\n",
    "                        sales.append(class_sales[0].text.strip())\n",
    "                    except:\n",
    "                        continue    \n",
    "                \n",
    "                    # Collecting the data of the Titles\n",
    "                    try:\n",
    "                        class_title = parsed.find_all(attrs={\"class\":re.compile(r\"andes-table__column--value\")})[0]\n",
    "                        titles.append( class_title.text.strip())\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "                 # Collecting the data of the Authors\n",
    "             \n",
    "                    try:\n",
    "                        class_author = parsed.find_all(attrs={\"class\":re.compile(r\"andes-table__column--value\")})[1]\n",
    "                        author.append(class_author.text.strip())\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "               # Collecting the data of the Language\n",
    "                    try:\n",
    "                        class_language = parsed.find_all(attrs={\"class\":re.compile(r\"andes-table__column--value\")})[2]\n",
    "                        language.append(class_language.text.strip())\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "               # Collecting the data of the Publisher\n",
    "                    try:\n",
    "                        class_editora = parsed.find_all(attrs={\"class\":re.compile(r\"andes-table__column--value\")})[3]\n",
    "                        editora.append(class_editora.text.strip())\n",
    "                    except:\n",
    "                        continue    \n",
    "            \n",
    "               # Collecting the data of the Format\n",
    "                    try:\n",
    "                        class_format = parsed.find_all(attrs={\"class\":re.compile(r\"andes-table__column--value\")})[5]\n",
    "                        formato.append(class_format.text.strip())\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "               # Collecting the data of the Subcategory\n",
    "                    try:\n",
    "                        class_subgen = parsed.find_all(attrs={\"class\":re.compile(r\"andes-breadcrumb__link\")})[2]\n",
    "                        subgen.append(class_subgen.text.strip())\n",
    "                    except:\n",
    "                        continue\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
